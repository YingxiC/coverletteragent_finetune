# --- Model and Dataset Configuration ---
model:
  # The Hugging Face model ID of the base model to fine-tune.
  name: "Qwen/Qwen3-0.6B-Base"
  # The Hugging Face dataset ID to use for training.
  dataset_name: "ShashiVish/cover-letter-dataset"
  # The name/path for the resulting fine-tuned model.
  new_model_name: "qwen-3-0.6b-finetuned"
  # Device map for model loading. "auto" is best for macOS/MPS.
  device_map: "auto"

data:
  train_file: "train.jsonl"
  val_file: "val.jsonl"
  dataset_format: "json"

# --- QLoRA (Quantized Low-Rank Adaptation) Configuration ---
lora:
  # LoRA attention dimension (rank). Higher means more parameters to train.
  r: 64
  # Alpha parameter for LoRA scaling.
  alpha: 16
  # Dropout probability for LoRA layers.
  dropout: 0.1

# --- BitsAndBytes (Quantization) Configuration ---
bitsandbytes:
  # Activate 4-bit precision base model loading.
  # Set to false if using a pre-quantized model (like GPTQ/AWQ).
  use_4bit: false
  # Compute dtype for 4-bit base models (float16 or bfloat16).
  compute_dtype: "float16"
  # Quantization type (fp4 or nf4). nf4 is usually better for LLMs.
  quant_type: "nf4"
  # Activate nested quantization for 4-bit base models (double quantization).
  use_nested_quant: false

# --- Training Arguments ---
training:
  # Output directory where the model predictions and checkpoints will be written.
  output_dir: "./results"
  # Number of training epochs.
  num_train_epochs: 1
  # Enable fp16/bf16 training (set bf16 to true for A100).
  fp16: false
  bf16: false
  # Batch size per GPU for training.
  per_device_train_batch_size: 4
  # Batch size per GPU for evaluation.
  per_device_eval_batch_size: 4
  # Number of update steps to accumulate the gradients for.
  gradient_accumulation_steps: 1
  # Enable gradient checkpointing to save memory at the cost of slower backward pass.
  gradient_checkpointing: true
  # Maximum gradient normal (gradient clipping).
  max_grad_norm: 0.3
  # Initial learning rate (AdamW optimizer).
  learning_rate: 0.0002
  # Weight decay to apply to all layers except bias/LayerNorm weights.
  weight_decay: 0.001
  # Optimizer to use.
  optim: "adamw_torch"
  # Learning rate schedule.
  lr_scheduler_type: "cosine"
  # Number of training steps (overrides num_train_epochs). -1 means use epochs.
  max_steps: -1
  # Ratio of steps for a linear warmup (from 0 to learning rate).
  warmup_ratio: 0.03
  # Group sequences into batches with same length. Saves memory and speeds up training considerably.
  group_by_length: true
  # Save checkpoint every X updates steps.
  save_steps: 0
  # Log every X updates steps.
  logging_steps: 25

# --- SFT (Supervised Fine-Tuning) Parameters ---
sft:
  # Maximum sequence length to use. None means it will be inferred or default to model max.
  max_seq_length: null
  # Pack multiple short examples into the same input sequence to increase efficiency.
  packing: false
